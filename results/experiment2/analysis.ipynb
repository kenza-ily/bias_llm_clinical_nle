{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to restart the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-6s9rn7bs/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "%pip install --quiet WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "import re\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df=pd.read_csv(\"/home/ucabkbe/bias_llm_clinical_challenge/results_20240813_185547_haiku4_2cases/exp2_GxE_llm_haiku.csv\")\n",
    "df = df.assign(experiment='gender')\n",
    "print(f\"Size of the df {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"Total input tokens {df['llm_gpt3_prompt_tokens_1'].sum()}\")\n",
    "print(f\"Total output tokens {df['llm_gpt3_completion_tokens_1'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df['llm_gpt3_response1'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"Average number of input token {df['llm_gpt3_prompt_tokens_1'].mean()}\")\n",
    "print(f\"Average number of output token {df['llm_gpt3_completion_tokens_1'].mean()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df['explanation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "PRICE_PER_OUTPUT_TOKEN=df['llm_gpt3_output_price_1'][0]/df['llm_gpt3_completion_tokens_1'][0]\n",
    "print(f\"Price per output token {PRICE_PER_OUTPUT_TOKEN}\")\n",
    "print(\"Price if the output token is 1000\", PRICE_PER_OUTPUT_TOKEN*1000)\n",
    "print(f\"Price for the whole df {PRICE_PER_OUTPUT_TOKEN*1000*1525}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MCQ Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Before postprocessing\n",
    "print(\"------ Before postprocessing ------\")\n",
    "print(f\"Unique values of label1: {df['llm_gpt3_label1'].unique()}\")\n",
    "\n",
    "\n",
    "# Postprocessing\n",
    "print(\"------ Postprocessing ------\")\n",
    "def extract_abcd_label(input_string):\n",
    "    if input_string is None or not isinstance(input_string, str):\n",
    "        return None\n",
    "    \n",
    "    # Convert to uppercase to simplify matching\n",
    "    input_string = input_string.upper()\n",
    "    \n",
    "    # Patterns to match A, B, C, or D in various contexts\n",
    "    patterns = [\n",
    "        r'\\b([ABCD])\\b',  # Standalone A, B, C, or D\n",
    "        r'(?:CORRECT|APPROPRIATE|DIAGNOSIS|ANSWER|OPTION)(?:\\s+\\w+){0,3}\\s+(?:IS|IN|:)?\\s*([ABCD])',  # \"correct answer is: X\" pattern\n",
    "        r'([ABCD])[\\s.:]',  # A, B, C, or D followed by space, period, or colon\n",
    "        r'\\n([ABCD])\\.?',  # A, B, C, or D at the start of a new line\n",
    "        r'([ABCD])(?=\\))',  # A, B, C, or D followed by a closing parenthesis\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, input_string)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    \n",
    "    # If no match found, find the first A, B, C, or D in the string\n",
    "    letters = re.findall(r'[ABCD]', input_string)\n",
    "    if letters:\n",
    "        return letters[0]\n",
    "    \n",
    "    # If still no match, return the first letter in the string (as a last resort)\n",
    "    letters = re.findall(r'[A-Z]', input_string)\n",
    "    return letters[0] if letters else None\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "df['llm_gpt3_label1'] = df['llm_gpt3_response1'].apply(extract_abcd_label)\n",
    "\n",
    "# Check for any remaining null values and print them for review\n",
    "null_responses = df[df['llm_gpt3_label1'].isnull()]['llm_gpt3_response1']\n",
    "if not null_responses.empty:\n",
    "    print(\"Responses without extracted labels:\")\n",
    "    print(null_responses)\n",
    "else:\n",
    "    print(\"All responses have been assigned a label.\")\n",
    "\n",
    "# After postprocessing\n",
    "print(\"------ Postprocessing ------\")\n",
    "print(f\"Unique values of label1: {df['llm_gpt3_label1'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['answer_idx_shuffled'].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. MCQ random evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.countplot(x='llm_gpt3_label1', data=df)\n",
    "plt.title('Count of Responses for Each Letter')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(df['llm_gpt3_label1'].unique())\n",
    "print(df['answer_idx_shuffled'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Group by 'field' and perform calculations for each group\n",
    "for field_value, group in df.groupby('field'):\n",
    "    # Calculate accuracy\n",
    "    group['correct'] = group['llm_gpt3_label1'].str.lower() == group['answer_idx_shuffled'].str.lower()\n",
    "    accuracy = group['correct'].mean()\n",
    "\n",
    "    # Count NaN values in llm_gpt3_label1\n",
    "    nan_count = group['llm_gpt3_label1'].isnull().sum()\n",
    "    \n",
    "    total_count=len(group['llm_gpt3_label1'])\n",
    "\n",
    "    # Calculate percentage of each gender\n",
    "    gender_counts = group['gender'].value_counts(normalize=True) * 100\n",
    "    percentage_male = gender_counts.get('male', 0)\n",
    "    percentage_female = gender_counts.get('female', 0)\n",
    "    percentage_neutral = gender_counts.get('neutral', 0)\n",
    "\n",
    "    # Append the results to the list\n",
    "    results_list.append({\n",
    "        'field': field_value,\n",
    "        'accuracy': accuracy * 100,\n",
    "        'number of NaN': nan_count,\n",
    "        'total lenght':total_count,\n",
    "        'percentage of male': percentage_male,\n",
    "        'percentage of female': percentage_female,\n",
    "        'percentage of neutral': percentage_neutral\n",
    "    })\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Print the results\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Capitalize df['answer_idx_shuffled']\n",
    "df['answer_idx_shuffled'] = df['answer_idx_shuffled'].str.upper()\n",
    "\n",
    "# Count the occurrences of each letter in answer_idx_shuffled and llm_gpt3_label1\n",
    "correct_counts = df['answer_idx_shuffled'].value_counts()\n",
    "model_counts = df['llm_gpt3_label1'].value_counts()\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Correct Answer': correct_counts,\n",
    "    'Model Selection': model_counts\n",
    "})\n",
    "\n",
    "# Fill any missing values with 0\n",
    "plot_data = plot_data.fillna(0)\n",
    "\n",
    "# Reset the index to make 'A', 'B', 'C', 'D' a column\n",
    "plot_data = plot_data.reset_index()\n",
    "plot_data.columns = ['Answer', 'Correct Answer', 'Model Selection']\n",
    "\n",
    "# Melt the DataFrame for easier plotting\n",
    "plot_data_melted = pd.melt(plot_data, id_vars=['Answer'], var_name='Type', value_name='Count')\n",
    "\n",
    "# Define custom colors\n",
    "color_dict = {\n",
    "    'Correct Answer': '#2ecc71',  # Green color\n",
    "    'Model Selection': '#3498db'  # Blue color (you can change this to any color you prefer)\n",
    "}\n",
    "\n",
    "# Create the seaborn histogram with custom colors\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Answer', y='Count', hue='Type', data=plot_data_melted, palette=color_dict)\n",
    "plt.title('Comparison of Correct Answers vs Model Selections')\n",
    "plt.xlabel('Answer Option')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Adjust legend\n",
    "plt.legend(title='', loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print the counts for reference\n",
    "print(\"Counts:\")\n",
    "print(plot_data)\n",
    "\n",
    "# Calculate and print the accuracy for each answer option\n",
    "accuracy = (df['answer_idx_shuffled'] == df['llm_gpt3_label1']).mean()\n",
    "print(f\"\\nOverall accuracy: {accuracy:.2%}\")\n",
    "\n",
    "accuracy_by_option = df.groupby('answer_idx_shuffled').apply(lambda x: (x['answer_idx_shuffled'] == x['llm_gpt3_label1']).mean())\n",
    "print(\"\\nAccuracy by answer option:\")\n",
    "print(accuracy_by_option)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Calculate accuracy\n",
    "df['correct'] = df['llm_gpt3_label1'] == df['answer_idx_shuffled']\n",
    "accuracy = df['correct'].mean()\n",
    "\n",
    "# Count NaN values in llm_gpt3_label1\n",
    "nan_count = df['llm_gpt3_label1'].isnull().sum()\n",
    "\n",
    "# Calculate percentage of each gender\n",
    "gender_percentage = df['gender'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Number of NaN values in llm_gpt3_label1: {nan_count}\")\n",
    "print(\"Percentage of each gender:\")\n",
    "print(gender_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Performance and Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Capitalize df['answer_idx_shuffled']\n",
    "df['answer_idx_shuffled'] = df['answer_idx_shuffled'].str.upper()\n",
    "\n",
    "# Define custom colors\n",
    "color_dict = {\n",
    "    'Correct Answer': '#2ecc71',  # Green color\n",
    "    'Model Selection': '#3498db'  # Blue color (you can change this to any color you prefer)\n",
    "}\n",
    "\n",
    "# Create a figure with 3 subplots side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('Comparison of Correct Answers vs Model Selections by Gender', fontsize=16)\n",
    "\n",
    "# Loop through each gender\n",
    "for i, gender in enumerate(df['gender'].unique()):\n",
    "    # Filter data for the current gender\n",
    "    gender_df = df[df['gender'] == gender]\n",
    "    \n",
    "    # Count the occurrences of each letter in answer_idx_shuffled and llm_gpt3_label1\n",
    "    correct_counts = gender_df['answer_idx_shuffled'].value_counts()\n",
    "    model_counts = gender_df['llm_gpt3_label1'].value_counts()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Correct Answer': correct_counts,\n",
    "        'Model Selection': model_counts\n",
    "    })\n",
    "\n",
    "    # Fill any missing values with 0 and ensure all options (A, B, C, D) are present\n",
    "    plot_data = plot_data.reindex(['A', 'B', 'C', 'D']).fillna(0)\n",
    "\n",
    "    # Reset the index to make 'A', 'B', 'C', 'D' a column\n",
    "    plot_data = plot_data.reset_index()\n",
    "    plot_data.columns = ['Answer', 'Correct Answer', 'Model Selection']\n",
    "\n",
    "    # Melt the DataFrame for easier plotting\n",
    "    plot_data_melted = pd.melt(plot_data, id_vars=['Answer'], var_name='Type', value_name='Count')\n",
    "\n",
    "    # Create the seaborn histogram with custom colors\n",
    "    sns.barplot(x='Answer', y='Count', hue='Type', data=plot_data_melted, palette=color_dict, ax=axes[i])\n",
    "    axes[i].set_title(f'Gender: {gender}')\n",
    "    axes[i].set_xlabel('Answer Option')\n",
    "    if i == 0:  # Only set ylabel for the first subplot\n",
    "        axes[i].set_ylabel('Count')\n",
    "    else:\n",
    "        axes[i].set_ylabel('')\n",
    "    \n",
    "    # Adjust legend\n",
    "    axes[i].legend(title='', loc='upper right')\n",
    "    \n",
    "    # Calculate and print the accuracy for each answer option\n",
    "    accuracy = (gender_df['answer_idx_shuffled'] == gender_df['llm_gpt3_label1']).mean()\n",
    "    print(f\"\\nGender: {gender}\")\n",
    "    print(f\"Overall accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    accuracy_by_option = gender_df.groupby('answer_idx_shuffled').apply(lambda x: (x['answer_idx_shuffled'] == x['llm_gpt3_label1']).mean())\n",
    "    print(\"Accuracy by answer option:\")\n",
    "    print(accuracy_by_option)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate and print accuracy by gender and overall\n",
    "print(\"\\nAccuracy by gender:\")\n",
    "for gender in df['gender'].unique():\n",
    "    gender_df = df[df['gender'] == gender]\n",
    "    gender_accuracy = (gender_df['answer_idx_shuffled'] == gender_df['llm_gpt3_label1']).mean()\n",
    "    gender_count = len(gender_df)\n",
    "    print(f\"{gender}: {gender_accuracy:.2%} (n={gender_count})\")\n",
    "\n",
    "overall_accuracy = (df['answer_idx_shuffled'] == df['llm_gpt3_label1']).mean()\n",
    "print(f\"\\nOverall accuracy across all genders: {overall_accuracy:.2%} (n={len(df)})\")\n",
    "\n",
    "# Calculate chi-square test for independence\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "contingency_table = pd.crosstab(df['gender'], df['answer_idx_shuffled'] == df['llm_gpt3_label1'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-square test for independence:\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(\"Null hypothesis: There is no significant difference in accuracy across genders\")\n",
    "print(f\"{'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis (α = 0.05)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4. Performance and version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Capitalize df['answer_idx_shuffled']\n",
    "# df['answer_idx_shuffled'] = df['answer_idx_shuffled'].str.upper()\n",
    "\n",
    "# # Define custom colors\n",
    "# color_dict = {\n",
    "#     'Correct Answer': '#2ecc71',  # Green color\n",
    "#     'Model Selection': '#3498db'  # Blue color (you can change this to any color you prefer)\n",
    "# }\n",
    "\n",
    "# # Get unique versions and calculate the number of subplots needed\n",
    "# versions = df['version'].unique()\n",
    "# n_versions = len(versions)\n",
    "\n",
    "# # Create a figure with subplots (adjust the layout based on the number of versions)\n",
    "# n_cols = min(3, n_versions)  # Maximum 3 columns\n",
    "# n_rows = (n_versions - 1) // 3 + 1\n",
    "# fig, axes = plt.subplots(n_rows, n_cols, figsize=(7*n_cols, 6*n_rows), sharey=True)\n",
    "# fig.suptitle('Comparison of Correct Answers vs Model Selections by Version', fontsize=16)\n",
    "\n",
    "# # Flatten axes array for easier indexing if there's only one row\n",
    "# if n_rows == 1:\n",
    "#     axes = axes.reshape(1, -1)\n",
    "\n",
    "# # Loop through each version\n",
    "# for i, version in enumerate(versions):\n",
    "#     row = i // 3\n",
    "#     col = i % 3\n",
    "    \n",
    "#     # Filter data for the current version\n",
    "#     version_df = df[df['version'] == version]\n",
    "    \n",
    "#     # Count the occurrences of each letter in answer_idx_shuffled and llm_gpt3_label1\n",
    "#     correct_counts = version_df['answer_idx_shuffled'].value_counts()\n",
    "#     model_counts = version_df['llm_gpt3_label1'].value_counts()\n",
    "\n",
    "#     # Create a DataFrame for plotting\n",
    "#     plot_data = pd.DataFrame({\n",
    "#         'Correct Answer': correct_counts,\n",
    "#         'Model Selection': model_counts\n",
    "#     })\n",
    "\n",
    "#     # Fill any missing values with 0 and ensure all options (A, B, C, D) are present\n",
    "#     plot_data = plot_data.reindex(['A', 'B', 'C', 'D']).fillna(0)\n",
    "\n",
    "#     # Reset the index to make 'A', 'B', 'C', 'D' a column\n",
    "#     plot_data = plot_data.reset_index()\n",
    "#     plot_data.columns = ['Answer', 'Correct Answer', 'Model Selection']\n",
    "\n",
    "#     # Melt the DataFrame for easier plotting\n",
    "#     plot_data_melted = pd.melt(plot_data, id_vars=['Answer'], var_name='Type', value_name='Count')\n",
    "\n",
    "#     # Create the seaborn histogram with custom colors\n",
    "#     sns.barplot(x='Answer', y='Count', hue='Type', data=plot_data_melted, palette=color_dict, ax=axes[row, col])\n",
    "#     axes[row, col].set_title(f'Version: {version}')\n",
    "#     axes[row, col].set_xlabel('Answer Option')\n",
    "#     if col == 0:  # Only set ylabel for the first subplot in each row\n",
    "#         axes[row, col].set_ylabel('Count')\n",
    "#     else:\n",
    "#         axes[row, col].set_ylabel('')\n",
    "    \n",
    "#     # Adjust legend\n",
    "#     axes[row, col].legend(title='', loc='upper right')\n",
    "\n",
    "# # Remove any unused subplots\n",
    "# for i in range(n_versions, n_rows * n_cols):\n",
    "#     row = i // 3\n",
    "#     col = i % 3\n",
    "#     fig.delaxes(axes[row, col])\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate and print accuracy by version and overall\n",
    "# print(\"\\nAccuracy by version:\")\n",
    "# for version in versions:\n",
    "#     version_df = df[df['version'] == version]\n",
    "#     version_accuracy = (version_df['answer_idx_shuffled'] == version_df['llm_gpt3_label1']).mean()\n",
    "#     version_count = len(version_df)\n",
    "#     print(f\"{version}: {version_accuracy:.2%} (n={version_count})\")\n",
    "\n",
    "# overall_accuracy = (df['answer_idx_shuffled'] == df['llm_gpt3_label1']).mean()\n",
    "# print(f\"\\nOverall accuracy across all versions: {overall_accuracy:.2%} (n={len(df)})\")\n",
    "\n",
    "# # Calculate chi-square test for independence\n",
    "# contingency_table = pd.crosstab(df['version'], df['answer_idx_shuffled'] == df['llm_gpt3_label1'])\n",
    "# chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# print(f\"\\nChi-square test for independence:\")\n",
    "# print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "# print(f\"p-value: {p_value:.4f}\")\n",
    "# print(\"Null hypothesis: There is no significant difference in accuracy across versions\")\n",
    "# print(f\"{'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis (α = 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Create the binary version column\n",
    "df['version_binary'] = df['version'].apply(lambda x: 'original' if x == 'original' else 'augmented')\n",
    "\n",
    "# Capitalize df['answer_idx_shuffled']\n",
    "df['answer_idx_shuffled'] = df['answer_idx_shuffled'].str.upper()\n",
    "\n",
    "# Define custom colors\n",
    "color_dict = {\n",
    "    'Correct Answer': '#2ecc71',  # Green color\n",
    "    'Model Selection': '#3498db'  # Blue color (you can change this to any color you prefer)\n",
    "}\n",
    "\n",
    "# Create a figure with 2 subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "fig.suptitle('Comparison of Correct Answers vs Model Selections by Version Type', fontsize=16)\n",
    "\n",
    "# Loop through each binary version\n",
    "for i, version in enumerate(['original', 'augmented']):\n",
    "    # Filter data for the current version\n",
    "    version_df = df[df['version_binary'] == version]\n",
    "    \n",
    "    # Count the occurrences of each letter in answer_idx_shuffled and llm_gpt3_label1\n",
    "    correct_counts = version_df['answer_idx_shuffled'].value_counts()\n",
    "    model_counts = version_df['llm_gpt3_label1'].value_counts()\n",
    "\n",
    "    # Create a DataFrame for plotting\n",
    "    plot_data = pd.DataFrame({\n",
    "        'Correct Answer': correct_counts,\n",
    "        'Model Selection': model_counts\n",
    "    })\n",
    "\n",
    "    # Fill any missing values with 0 and ensure all options (A, B, C, D) are present\n",
    "    plot_data = plot_data.reindex(['A', 'B', 'C', 'D']).fillna(0)\n",
    "\n",
    "    # Reset the index to make 'A', 'B', 'C', 'D' a column\n",
    "    plot_data = plot_data.reset_index()\n",
    "    plot_data.columns = ['Answer', 'Correct Answer', 'Model Selection']\n",
    "\n",
    "    # Melt the DataFrame for easier plotting\n",
    "    plot_data_melted = pd.melt(plot_data, id_vars=['Answer'], var_name='Type', value_name='Count')\n",
    "\n",
    "    # Create the seaborn histogram with custom colors\n",
    "    sns.barplot(x='Answer', y='Count', hue='Type', data=plot_data_melted, palette=color_dict, ax=axes[i])\n",
    "    axes[i].set_title(f'Version: {version.capitalize()}')\n",
    "    axes[i].set_xlabel('Answer Option')\n",
    "    if i == 0:  # Only set ylabel for the first subplot\n",
    "        axes[i].set_ylabel('Count')\n",
    "    else:\n",
    "        axes[i].set_ylabel('')\n",
    "    \n",
    "    # Adjust legend\n",
    "    axes[i].legend(title='', loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print accuracy by binary version and overall\n",
    "print(\"\\nAccuracy by version type:\")\n",
    "for version in ['original', 'augmented']:\n",
    "    version_df = df[df['version_binary'] == version]\n",
    "    version_accuracy = (version_df['answer_idx_shuffled'] == version_df['llm_gpt3_label1']).mean()\n",
    "    version_count = len(version_df)\n",
    "    print(f\"{version.capitalize()}: {version_accuracy:.2%} (n={version_count})\")\n",
    "\n",
    "overall_accuracy = (df['answer_idx_shuffled'] == df['llm_gpt3_label1']).mean()\n",
    "print(f\"\\nOverall accuracy across all versions: {overall_accuracy:.2%} (n={len(df)})\")\n",
    "\n",
    "# Calculate chi-square test for independence\n",
    "contingency_table = pd.crosstab(df['version_binary'], df['answer_idx_shuffled'] == df['llm_gpt3_label1'])\n",
    "chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-square test for independence:\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(\"Null hypothesis: There is no significant difference in accuracy between original and augmented versions\")\n",
    "print(f\"{'Reject' if p_value < 0.05 else 'Fail to reject'} the null hypothesis (α = 0.05)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gender specific question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Before postprocessing\n",
    "# print(\"------ Before postprocessing ------\")\n",
    "# print(f\"Unique values of label2: {df['llm_gpt3_label2'].unique()}\")\n",
    "\n",
    "\n",
    "# # Postprocessing\n",
    "# print(\"------ Postprocessing ------\")\n",
    "\n",
    "# def extract_ranking(value):\n",
    "#     # Try to find a single digit at the start of the string\n",
    "#     match = re.match(r'^(\\d)', str(value))\n",
    "#     if match:\n",
    "#         return int(match.group(1))\n",
    "    \n",
    "#     # If not found, search for any single digit in the string\n",
    "#     match = re.search(r'\\b(\\d)\\b', str(value))\n",
    "#     if match:\n",
    "#         return int(match.group(1))\n",
    "    \n",
    "#     # If still not found, return None\n",
    "#     return None\n",
    "\n",
    "# def process_array(arr):\n",
    "#     return [extract_ranking(value) for value in arr]\n",
    "\n",
    "\n",
    "# df['llm_gpt3_label2'] = df['llm_gpt3_label2'].apply(extract_ranking)\n",
    "\n",
    "# # Human postprocessing\n",
    "# df.at[5, 'llm_gpt3_label2'] = 0\n",
    "# df.at[41, 'llm_gpt3_label2'] = 0\n",
    "\n",
    "\n",
    "# # Check for any remaining null values and print them for review\n",
    "# null_responses = df[df['llm_gpt3_label2'].isnull()]['llm_gpt3_response2']\n",
    "# if not null_responses.empty:\n",
    "#     print(\"Responses without extracted labels:\")\n",
    "#     print(null_responses)\n",
    "# else:\n",
    "#     print(\"All responses have been assigned a label.\")\n",
    "\n",
    "# # After postprocessing\n",
    "# print(\"------ Postprocessing ------\")\n",
    "# print(f\"Unique values of label2: {df['llm_gpt3_label2'].unique()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# This code snippet is using the Python libraries Seaborn and Matplotlib to create a countplot.\n",
    "# # import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(10,7))\n",
    "# sns.countplot(x='llm_gpt3_label2', data=df)\n",
    "# plt.title('Count of Responses for Each Letter')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 7))\n",
    "# sns.countplot(x='llm_gpt3_label2', hue='gender', data=df)\n",
    "# plt.title('Count of Responses for Each Letter by Gender')\n",
    "# plt.xlabel('Letter')\n",
    "# plt.ylabel('Count')\n",
    "# plt.legend(title='Gender')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 7))\n",
    "# sns.countplot(x='llm_gpt3_label2', hue='version', data=df)\n",
    "# plt.title('Count of Responses for Each Letter by Gender')\n",
    "# plt.xlabel('Letter')\n",
    "# plt.ylabel('Count')\n",
    "# plt.legend(title='Gender')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and gender and version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy per gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ========= GENDER IMPORTANCE =========\n",
    "\n",
    "# Calculate the average performance and standard error for each gender\n",
    "gender_importance = df.groupby('gender')['llm_gpt3_label2'].agg(['mean', 'sem'])\n",
    "\n",
    "# Create a bar plot\n",
    "# Plot bars\n",
    "bars = ax.bar(gender_importance.index, gender_importance['mean'], \n",
    "              color=['blue', 'pink'], alpha=0.7)\n",
    "\n",
    "# Add error bars\n",
    "ax.errorbar(gender_importance.index, gender_importance['mean'], \n",
    "            yerr=gender_importance['sem'], fmt='none', color='black', capsize=5)\n",
    "\n",
    "# Set plot title and labels\n",
    "ax.set_title('Average Performance by Gender with Error Bars')\n",
    "ax.set_xlabel('Gender')\n",
    "ax.set_ylabel('Average Performance')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ===== PERFORMANCE EXPlANATION =====\n",
    "\n",
    "# Combine all text from the column into a single string\n",
    "text = ' '.join(df['llm_gpt3_response1'].dropna().astype(str))\n",
    "\n",
    "# Create and generate a word cloud image\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "\n",
    "# Display the generated image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of llm_gpt3_response1')\n",
    "plt.tight_layout(pad=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_cols = [col for col in df.columns if df[col].dtypes != \"O\"]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def num_summary(dataframe, numerical_col, plot=False):\n",
    "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
    "    print(dataframe[numerical_col].describe(quantiles).T)\n",
    "\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(\n",
    "            1, 2, figsize=(14, 5)\n",
    "        )  # 1 row, 2 columns for two subplots\n",
    "        dataframe[numerical_col].hist(bins=20, ax=ax[0])\n",
    "        ax[0].set_xlabel(numerical_col)\n",
    "        ax[0].set_title(numerical_col)\n",
    "\n",
    "        # Add more plots as needed\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for better spacing\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Assuming df and num_cols are defined somewhere before calling num_summary\n",
    "num_summary(df, num_cols, plot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for i, col in enumerate(num_cols):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    sns.histplot(data=df[col], color=\"darkblue\", kde=True, bins=30)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "\n",
    "# removing empty subplots\n",
    "num_columns = len(num_cols)\n",
    "num_rows = num_columns // 3 + (1 if num_columns % 3 != 0 else 0)\n",
    "for i in range(num_columns, num_rows * 3):\n",
    "    fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "fig.suptitle(\"Numerical Univariate Variable Analysis\", fontsize=20, y=1.02)\n",
    "plt.subplots_adjust(top=1.88)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['llm_gpt3_performance'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Assuming df and num_cols are already defined\n",
    "\n",
    "# Calculate the number of rows and columns needed\n",
    "n_features = len(df[num_cols]) - 1\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_features / n_cols)\n",
    "\n",
    "# Set up the figure size\n",
    "figsize = (6 * n_cols, 5 * n_rows)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(df[num_cols][:-1]):\n",
    "    ax = axes[idx]\n",
    "    sns.kdeplot(data=df, hue=\"llm_gpt3_performance\", fill=True, x=col, legend=False, ax=ax)\n",
    "    \n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_title(f\"{col}\", loc=\"right\", weight=\"bold\", fontsize=12)\n",
    "\n",
    "# Remove any unused subplots\n",
    "for idx in range(n_features, len(axes)):\n",
    "    fig.delaxes(axes[idx])\n",
    "\n",
    "fig.suptitle(f\"Features vs Target\", ha=\"center\", fontweight=\"bold\", fontsize=16)\n",
    "fig.legend(\n",
    "    [\"correct\", \"incorrect\"],\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 0.98),\n",
    "    fontsize=12,\n",
    "    ncol=2,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-xnh_qjzt/argon2-cffi-bindings/. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
