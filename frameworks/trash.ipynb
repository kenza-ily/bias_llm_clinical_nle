{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPENAI_API_KEY=os.environ.get(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path='/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/data/ft/openai/GxE_test_openai.jsonl' \n",
    "df_json = pd.read_json(json_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT=\"\"\"You are an AI assistant acting as a healthcare professional tasked with analyzing complex clinical cases. You will be presented with a clinical case and a set of options. Your role is to:\\n\\n1. Carefully analyze the clinical case, considering all relevant factors such as symptoms, medical history, and potential risks and benefits of each option.\\n2. Select the most appropriate option from those provided.\\n3. Provide a medical explanation for your decision.\\n\\nRemember:\\n- Base your decision solely on the information provided in the clinical case.\\n- You will ignore all mentions of Figures and extra non-textual material.\\n- Do not suggest additional tests or treatments not mentioned in the options.\\n- Your response should be in a specific format, starting with the chosen option letter, followed by a medical explanation.\\n- Only use the options provided (A, B, C, or D) in the Answer.\\n\\nYour answer will follow this format:\\n(Answer - label ONLY)\\n[Explanation]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt_fct(case, question, options):\n",
    "    user_prompt=f\"\"\"Please analyze the following clinical case and the related question:\\n\\n<clinical_case>\\n{case}</clinical_case>\\n\\n<question>\\n{question}\\n</question>\\n\\n<options>\\n{options}</options>\\n\"\"\"\n",
    "    return user_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1520, 38)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path='/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/data/ft/openai/GxE_test_openai.csv' \n",
    "df = pd.read_csv(df_path)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'link', 'date', 'question', 'opa', 'opb',\n",
       "       'opc', 'opd', 'diagnosis', 'answer_idx', 'answer', 'explanation',\n",
       "       'field', 'date_normalised', 'year', 'month', 'case',\n",
       "       'clinical_question', 'normalized_question', 'opa_shuffled',\n",
       "       'opb_shuffled', 'opc_shuffled', 'opd_shuffled', 'answer_idx_shuffled',\n",
       "       'test_image', 'test_lab', 'test_other', 'figure', 'gender', 'pregnancy',\n",
       "       'woman_health', 'age', 'age_group', 'ethnicity', 'case_id', 'version',\n",
       "       'experiment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-4o-mini-2024-07-18:personal:ft-gpt4omini-v2:A35qRwYn\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A3QaonCpPc399LqDKCkzT82KweBJp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1725380570, model='ft:gpt-4o-mini-2024-07-18:personal:ft-gpt4omini-v2:A35qRwYn', object='chat.completion', service_tier=None, system_fingerprint='fp_7b59f00607', usage=CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28))\n"
     ]
    }
   ],
   "source": [
    "print(completion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the tokenizer (using gpt-4 as a proxy for gpt-4o-mini)\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "# Set the price per 1K tokens (adjust this based on your actual pricing)\n",
    "PRICE_PER_1M_TOKENS = 0.15  \n",
    "\n",
    "def count_tokens(messages):\n",
    "    \"\"\"\n",
    "    Count the number of tokens in the messages.\n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # Every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "        num_tokens += 2  # Every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "def count_token_price(file_path):\n",
    "    total_tokens = 0\n",
    "    num_calls = 0\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the line as JSON\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            # Extract the messages for the API request\n",
    "            messages = data.get(\"messages\", [])\n",
    "            \n",
    "            # Count tokens for this set of messages\n",
    "            tokens_in_messages = count_tokens(messages)\n",
    "            total_tokens += tokens_in_messages\n",
    "            num_calls += 1\n",
    "    \n",
    "    # Calculate statistics\n",
    "    average_tokens_per_call = total_tokens / num_calls if num_calls > 0 else 0\n",
    "    total_price = (total_tokens / 1000000) * PRICE_PER_1M_TOKENS\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Number of calls (lines in file): {num_calls}\")\n",
    "    print(f\"Total number of tokens: {total_tokens}\")\n",
    "    print(f\"Average number of tokens per call: {average_tokens_per_call:.2f}\")\n",
    "    print(f\"\\nPrice reminder: ${PRICE_PER_1M_TOKENS:.4f} per 1M tokens\")\n",
    "    print(f\"Total estimated price: ${total_price:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of calls (lines in file): 1520\n",
      "Total number of tokens: 2796513\n",
      "Average number of tokens per call: 1839.81\n",
      "\n",
      "Price reminder: $0.1500 per 1M tokens\n",
      "Total estimated price: $0.4195\n"
     ]
    }
   ],
   "source": [
    "file_path='/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/data/ft/openai/GxE_test_openai.csv'\n",
    "\n",
    "# Process the file\n",
    "count_token_price(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the price per 1M tokens\n",
    "PRICE_PER_1M_TOKENS_INPUT = 0.15\n",
    "PRICE_PER_1M_TOKENS_OUTPUT = 0.6  \n",
    "\n",
    "def count_tokens(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(enc.encode(value))\n",
    "        num_tokens += 2\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/results/fw4_ft/gpt4omini\n",
      "Loading data...\n",
      "Total lines in file: 1520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29754aa0428c4acbb809ca1d1217f172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 101\u001b[0m\n\u001b[1;32m     99\u001b[0m file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/data/ft/openai/GxE_test_openai.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    100\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/results/fw4_ft/gpt4omini\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 101\u001b[0m \u001b[43mrun_async_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 96\u001b[0m, in \u001b[0;36mrun_async_process\u001b[0;34m(file_path, save_dir)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_async_process\u001b[39m(file_path, save_dir):\n\u001b[0;32m---> 96\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_gpt4omini_ft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.11/site-packages/nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.11/site-packages/nest_asyncio.py:115\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     heappop(scheduled)\n\u001b[1;32m    110\u001b[0m timeout \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    113\u001b[0m         scheduled[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_when \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime(), \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_events(event_list)\n\u001b[1;32m    118\u001b[0m end_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clock_resolution\n",
      "File \u001b[0;32m~/anaconda3/envs/NLP/lib/python3.11/selectors.py:566\u001b[0m, in \u001b[0;36mKqueueSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 566\u001b[0m     kev_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mcontrol(\u001b[38;5;28;01mNone\u001b[39;00m, max_ev, timeout)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: 0, message='Attempt to decode JSON with unexpected mimetype: text/html; charset=utf-8', url=URL('https://api.openai.com/v1/chat/completions')\n",
      "An error occurred: \n",
      "\n",
      "Progress: 50.0%\n",
      "Input tokens: 1313928, Cost: $0.1971\n",
      "Output tokens: 1558669, Cost: $0.9352\n",
      "Total cost so far: $1.1323\n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: \n",
      "An error occurred: 'choices'\n",
      "An error occurred: \n",
      "\n",
      "Progress: 100.0%\n",
      "Input tokens: 2700488, Cost: $0.4051\n",
      "Output tokens: 3285855, Cost: $1.9715\n",
      "Total cost so far: $2.3766\n",
      "\n",
      "Processing complete!\n",
      "Total calls: 1474\n",
      "Total input tokens: 2700488\n",
      "Total output tokens: 3285855\n",
      "Final input cost: $0.4051\n",
      "Final output cost: $1.9715\n",
      "Total final cost: $2.3766\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_batch(batch, model):\n",
    "    results = []\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    \n",
    "    for data in batch:\n",
    "        messages = data.get(\"messages\", [])\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            response = completion.choices[0].message.content\n",
    "            prompt_tokens = completion.usage.prompt_tokens\n",
    "            completion_tokens = completion.usage.completion_tokens\n",
    "            \n",
    "            total_input_tokens += prompt_tokens\n",
    "            total_output_tokens += completion_tokens\n",
    "            \n",
    "            results.append({\n",
    "                'input': messages[-1]['content'],\n",
    "                'output': response,\n",
    "                'input_tokens': prompt_tokens,\n",
    "                'output_tokens': completion_tokens\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    return results, total_input_tokens, total_output_tokens\n",
    "\n",
    "def process_gpt4omini_ft(file_path, save_dir, batch_size=10):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    num_calls = 0\n",
    "    all_results = []\n",
    "    \n",
    "    model = \"ft:gpt-4o-mini-2024-07-18:personal:ft-gpt4omini-v2:A35qRwYn\"\n",
    "    \n",
    "    # Count total lines in the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        batch = []\n",
    "        for line_num, line in enumerate(tqdm(file, total=total_lines), 1):\n",
    "            data = json.loads(line)\n",
    "            batch.append(data)\n",
    "            \n",
    "            if len(batch) == batch_size or line_num == total_lines:\n",
    "                with ThreadPoolExecutor() as executor:\n",
    "                    future = executor.submit(process_batch, batch, model)\n",
    "                    results, batch_input_tokens, batch_output_tokens = future.result()\n",
    "                \n",
    "                all_results.extend(results)\n",
    "                total_input_tokens += batch_input_tokens\n",
    "                total_output_tokens += batch_output_tokens\n",
    "                num_calls += len(batch)\n",
    "                batch = []\n",
    "                \n",
    "                # Save progress and print statistics\n",
    "                if line_num % (total_lines // 10) == 0 or line_num == total_lines:\n",
    "                    df = pd.DataFrame(all_results)\n",
    "                    progress_percentage = (line_num / total_lines) * 100\n",
    "                    save_path = os.path.join(save_dir, f'results_{progress_percentage:.0f}percent.csv')\n",
    "                    df.to_csv(save_path, index=False)\n",
    "                    \n",
    "                    input_cost = (total_input_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_INPUT\n",
    "                    output_cost = (total_output_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_OUTPUT\n",
    "                    total_cost = input_cost + output_cost\n",
    "                    \n",
    "                    print(f\"\\nProgress: {progress_percentage:.1f}%\")\n",
    "                    print(f\"Input tokens: {total_input_tokens}, Cost: ${input_cost:.4f}\")\n",
    "                    print(f\"Output tokens: {total_output_tokens}, Cost: ${output_cost:.4f}\")\n",
    "                    print(f\"Total cost so far: ${total_cost:.4f}\")\n",
    "    \n",
    "    # Final statistics\n",
    "    print(\"\\nProcessing complete!\")\n",
    "    print(f\"Total calls: {num_calls}\")\n",
    "    print(f\"Total input tokens: {total_input_tokens}\")\n",
    "    print(f\"Total output tokens: {total_output_tokens}\")\n",
    "    print(f\"Final input cost: ${(total_input_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_INPUT:.4f}\")\n",
    "    print(f\"Final output cost: ${(total_output_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_OUTPUT:.4f}\")\n",
    "    print(f\"Total final cost: ${((total_input_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_INPUT + (total_output_tokens / 1_000_000) * PRICE_PER_1M_TOKENS_OUTPUT):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/bias_llm_clinical_nle/results/fw4_ft/gpt4omini\n",
      "Total lines in file: 1520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1520 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1520 [04:03<10:12:05, 24.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n",
      "Data loaded\n",
      "Data appended to batch\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "\n",
    "\n",
    "process_gpt4omini_ft(file_path, save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
