{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API connection\n",
    "Describe the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (24.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting python-dotenv (from -r requirements.txt (line 1))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting AzureOpenAI (from -r requirements.txt (line 2))\n",
      "  Using cached azureopenai-0.0.1-py2.py3-none-any.whl.metadata (128 bytes)\n",
      "Collecting langchain-openai (from -r requirements.txt (line 3))\n",
      "  Using cached langchain_openai-0.1.13-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pandas (from -r requirements.txt (line 4))\n",
      "  Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting numpy (from -r requirements.txt (line 5))\n",
      "  Using cached numpy-2.0.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting spacy (from -r requirements.txt (line 6))\n",
      "  Using cached spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 8))\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 9))\n",
      "  Using cached langchain-0.2.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting langsmith (from -r requirements.txt (line 10))\n",
      "  Using cached langsmith-0.1.83-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langchain-core (from -r requirements.txt (line 11))\n",
      "  Using cached langchain_core-0.2.10-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langchain-cohere (from -r requirements.txt (line 12))\n",
      "  Using cached langchain_cohere-0.1.8-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting langchain-mistralai (from -r requirements.txt (line 13))\n",
      "  Using cached langchain_mistralai-0.1.9-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting langchain-anthropic (from -r requirements.txt (line 14))\n",
      "  Downloading langchain_anthropic-0.1.17-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting matplotlib (from -r requirements.txt (line 15))\n",
      "  Using cached matplotlib-3.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting seaborn (from -r requirements.txt (line 16))\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting aiohttp (from AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.5 kB)\n",
      "Collecting jsonschema (from AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached jsonschema-4.22.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting requests (from AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting openai<2.0.0,>=1.32.0 (from langchain-openai->-r requirements.txt (line 3))\n",
      "  Downloading openai-1.35.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai->-r requirements.txt (line 3))\n",
      "  Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 4))\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->-r requirements.txt (line 4))\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy->-r requirements.txt (line 6))\n",
      "  Downloading pydantic-2.8.0-py3-none-any.whl.metadata (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m521.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy->-r requirements.txt (line 6)) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy->-r requirements.txt (line 6)) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy->-r requirements.txt (line 6))\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 8))\n",
      "  Downloading scipy-1.14.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 8))\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 8))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain->-r requirements.txt (line 9))\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->-r requirements.txt (line 9))\n",
      "  Using cached SQLAlchemy-2.0.31-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain->-r requirements.txt (line 9))\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting numpy (from -r requirements.txt (line 5))\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain->-r requirements.txt (line 9))\n",
      "  Downloading tenacity-8.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith->-r requirements.txt (line 10))\n",
      "  Using cached orjson-3.10.5-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (49 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core->-r requirements.txt (line 11))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere->-r requirements.txt (line 12))\n",
      "  Using cached cohere-5.5.8-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting httpx<1,>=0.25.2 (from langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting httpx-sse<1,>=0.3.1 (from langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tokenizers<1,>=0.15.1 (from langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting anthropic<1,>=0.28.0 (from langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Downloading anthropic-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting defusedxml<0.8.0,>=0.7.1 (from langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Using cached fonttools-4.53.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (162 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting pillow>=8 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 15))\n",
      "  Using cached pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (31 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from anthropic<1,>=0.28.0->langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from anthropic<1,>=0.28.0->langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from anthropic<1,>=0.28.0->langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Downloading jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting sniffio (from anthropic<1,>=0.28.0->langchain-anthropic->-r requirements.txt (line 14))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from anthropic<1,>=0.28.0->langchain-anthropic->-r requirements.txt (line 14)) (4.12.2)\n",
      "Collecting boto3<2.0.0,>=1.34.0 (from cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Downloading boto3-1.34.137-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Using cached fastavro-1.9.4-cp311-cp311-macosx_10_9_universal2.whl.metadata (5.5 kB)\n",
      "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Using cached parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Using cached types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting certifi (from httpx<1,>=0.25.2->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached certifi-2024.6.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.25.2->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx<1,>=0.25.2->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core->-r requirements.txt (line 11))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->-r requirements.txt (line 6))\n",
      "  Downloading pydantic_core-2.20.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 6))\n",
      "  Using cached blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy->-r requirements.txt (line 6))\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain-openai->-r requirements.txt (line 3))\n",
      "  Using cached regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers<1,>=0.15.1->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy->-r requirements.txt (line 6))\n",
      "  Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema->AzureOpenAI->-r requirements.txt (line 2))\n",
      "  Using cached rpds_py-0.18.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
      "Collecting botocore<1.35.0,>=1.34.137 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Downloading botocore-1.34.137-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere->-r requirements.txt (line 12))\n",
      "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15.1->langchain-mistralai->-r requirements.txt (line 13))\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6)) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy->-r requirements.txt (line 6))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached azureopenai-0.0.1-py2.py3-none-any.whl (966 bytes)\n",
      "Using cached langchain_openai-0.1.13-py3-none-any.whl (45 kB)\n",
      "Using cached pandas-2.2.2-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached spacy-3.7.5-cp311-cp311-macosx_11_0_arm64.whl (6.5 MB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached langchain-0.2.6-py3-none-any.whl (975 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached langsmith-0.1.83-py3-none-any.whl (127 kB)\n",
      "Using cached langchain_core-0.2.10-py3-none-any.whl (332 kB)\n",
      "Using cached langchain_cohere-0.1.8-py3-none-any.whl (31 kB)\n",
      "Using cached langchain_mistralai-0.1.9-py3-none-any.whl (13 kB)\n",
      "Downloading langchain_anthropic-0.1.17-py3-none-any.whl (19 kB)\n",
      "Using cached matplotlib-3.9.0-cp311-cp311-macosx_11_0_arm64.whl (7.8 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached aiohttp-3.9.5-cp311-cp311-macosx_11_0_arm64.whl (390 kB)\n",
      "Downloading anthropic-0.30.1-py3-none-any.whl (863 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.9/863.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cohere-5.5.8-py3-none-any.whl (173 kB)\n",
      "Using cached contourpy-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (245 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached fonttools-4.53.0-cp311-cp311-macosx_11_0_arm64.whl (2.2 MB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached kiwisolver-1.4.5-cp311-cp311-macosx_11_0_arm64.whl (66 kB)\n",
      "Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Using cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Downloading openai-1.35.7-py3-none-any.whl (327 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.5/327.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached orjson-3.10.5-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (258 kB)\n",
      "Downloading pillow-10.4.0-cp311-cp311-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "Downloading pydantic-2.8.0-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.0-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading scipy-1.14.0-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached SQLAlchemy-2.0.31-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "Downloading tenacity-8.4.2-py3-none-any.whl (28 kB)\n",
      "Using cached thinc-8.2.5-cp311-cp311-macosx_11_0_arm64.whl (773 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tiktoken-0.7.0-cp311-cp311-macosx_11_0_arm64.whl (907 kB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached jsonschema-4.22.0-py3-none-any.whl (88 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached blis-0.7.11-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Downloading boto3-1.34.137-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached certifi-2024.6.2-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fastavro-1.9.4-cp311-cp311-macosx_10_9_universal2.whl (1.1 MB)\n",
      "Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Downloading jiter-0.5.0-cp311-cp311-macosx_11_0_arm64.whl (299 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.3/299.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached MarkupSafe-2.1.5-cp311-cp311-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached rpds_py-0.18.1-cp311-cp311-macosx_11_0_arm64.whl (322 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Downloading botocore-1.34.137-py3-none-any.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, cymem, wrapt, wasabi, urllib3, tzdata, tqdm, threadpoolctl, tenacity, SQLAlchemy, spacy-loggers, spacy-legacy, sniffio, shellingham, rpds-py, regex, PyYAML, python-dotenv, pyparsing, pydantic-core, pillow, parameterized, orjson, numpy, murmurhash, multidict, mdurl, MarkupSafe, marisa-trie, kiwisolver, jsonpointer, joblib, jmespath, jiter, idna, httpx-sse, h11, fsspec, frozenlist, fonttools, filelock, fastavro, distro, defusedxml, cycler, cloudpathlib, click, charset-normalizer, certifi, catalogue, attrs, annotated-types, yarl, types-requests, srsly, smart-open, scipy, requests, referencing, pydantic, preshed, pandas, markdown-it-py, language-data, jsonpatch, jinja2, httpcore, contourpy, botocore, blis, anyio, aiosignal, tiktoken, scikit-learn, s3transfer, rich, matplotlib, langsmith, langcodes, jsonschema-specifications, huggingface-hub, httpx, confection, aiohttp, typer, tokenizers, thinc, seaborn, openai, langchain-core, jsonschema, boto3, weasel, langchain-text-splitters, langchain-openai, langchain-mistralai, cohere, AzureOpenAI, anthropic, spacy, langchain-cohere, langchain-anthropic, langchain\n",
      "Successfully installed AzureOpenAI-0.0.1 MarkupSafe-2.1.5 PyYAML-6.0.1 SQLAlchemy-2.0.31 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.7.0 anthropic-0.30.1 anyio-4.4.0 attrs-23.2.0 blis-0.7.11 boto3-1.34.137 botocore-1.34.137 catalogue-2.0.10 certifi-2024.6.2 charset-normalizer-3.3.2 click-8.1.7 cloudpathlib-0.18.1 cohere-5.5.8 confection-0.1.5 contourpy-1.2.1 cycler-0.12.1 cymem-2.0.8 defusedxml-0.7.1 distro-1.9.0 fastavro-1.9.4 filelock-3.15.4 fonttools-4.53.0 frozenlist-1.4.1 fsspec-2024.6.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 httpx-sse-0.4.0 huggingface-hub-0.23.4 idna-3.7 jinja2-3.1.4 jiter-0.5.0 jmespath-1.0.1 joblib-1.4.2 jsonpatch-1.33 jsonpointer-3.0.0 jsonschema-4.22.0 jsonschema-specifications-2023.12.1 kiwisolver-1.4.5 langchain-0.2.6 langchain-anthropic-0.1.17 langchain-cohere-0.1.8 langchain-core-0.2.10 langchain-mistralai-0.1.9 langchain-openai-0.1.13 langchain-text-splitters-0.2.2 langcodes-3.4.0 langsmith-0.1.83 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 matplotlib-3.9.0 mdurl-0.1.2 multidict-6.0.5 murmurhash-1.0.10 numpy-1.26.4 openai-1.35.7 orjson-3.10.5 pandas-2.2.2 parameterized-0.9.0 pillow-10.4.0 preshed-3.0.9 pydantic-2.8.0 pydantic-core-2.20.0 pyparsing-3.1.2 python-dotenv-1.0.1 pytz-2024.1 referencing-0.35.1 regex-2024.5.15 requests-2.32.3 rich-13.7.1 rpds-py-0.18.1 s3transfer-0.10.2 scikit-learn-1.5.0 scipy-1.14.0 seaborn-0.13.2 shellingham-1.5.4 smart-open-7.0.4 sniffio-1.3.1 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 tenacity-8.4.2 thinc-8.2.5 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 typer-0.12.3 types-requests-2.32.0.20240622 tzdata-2024.1 urllib3-2.2.2 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.0)\n",
      "Requirement already satisfied: jinja2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/thesis/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Imports \n",
    "\n",
    "\n",
    "# --- Python basics\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import time\n",
    "import re #to remove if enough done in the EDA step\n",
    "from datetime import datetime\n",
    "\n",
    "# --- ML basics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# --- LLMs basics\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "# from openai import AzureOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# ==== useless for now\n",
    "# import json\n",
    "# from collections import Counter\n",
    "# import itertools\n",
    "# import glob\n",
    "# import re\n",
    "# from ast import literal_eval\n",
    "# import typing as tp\n",
    "# import scipy.stats\n",
    "# import ast\n",
    "# import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs costs\n",
    "\n",
    "# == GPTs\n",
    "# --- GPT 3.5\n",
    "PRICE_PER_INPUT_TOKEN_GPT3=0.5/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT3=1.5/(1e6)\n",
    "# --- GPT 4o\n",
    "PRICE_PER_INPUT_TOKEN_GPT4O=5/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4O=15/(1e6)\n",
    "# --- GPT 4 Turbo\n",
    "PRICE_PER_INPUT_TOKEN_GPT4_TURBO=10/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4_TURBO=30/(1e6)\n",
    "# ---GPT 4\n",
    "PRICE_PER_INPUT_TOKEN_GPT4=30/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4=60/(1e6)\n",
    "\n",
    "# == Cohere\n",
    "# --- Command-R\n",
    "PRICE_PER_INPUT_TOKEN_COMMANDR=0.5/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_COMMANDR=1.5/(1e6)\n",
    "# --- Command-R-Plus\n",
    "PRICE_PER_INPUT_TOKEN_COMMANDR_PLUS=3/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_COMMANDR_PLUS=15/(1e6)\n",
    "\n",
    "# == Claude\n",
    "# --- Haiku\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_HAIKU=0.25/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_HAIKU=1.25/(1e6)\n",
    "# --- Sonnet\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_SONNET=3/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_SONNET=15/(1e6)\n",
    "\n",
    "# --- Opus\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_OPUS=15/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_OPUS=75/(1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== GET THE API KEYS\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from `keys.env`\n",
    "load_dotenv('keys.env')\n",
    "\n",
    "# ---- OpenAI\n",
    "# Accessing the variables\n",
    "bias1_key1 = os.getenv('BIAS1_KEY1')\n",
    "azure_openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_openai_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "azure_openai_api_version = os.getenv('AZURE_OPENAI_API_VERSION')\n",
    "azure_openai_chat_deployment_name = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs hyperparameters\n",
    "TEMPERATURE=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs loading\n",
    "\n",
    "# ------ GPT3.5\n",
    "llm_gpt3 = llm = AzureChatOpenAI(\n",
    "    openai_api_version=azure_openai_api_version,\n",
    "    azure_deployment=azure_openai_chat_deployment_name,\n",
    "    temperature=TEMPERATURE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT4-turbo not in Azure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLM dictionnary definition\n",
    "\n",
    "llms = {\n",
    "    \"llm_gpt3\": {\n",
    "        \"model\": llm_gpt3,\n",
    "        \"model_name\":llm_gpt3.model_name,\n",
    "        \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_GPT3,\n",
    "        \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_GPT3\n",
    "    }\n",
    "    #},\n",
    "    # \"llm_gpt4\": {\n",
    "    #     \"model\": llm_gpt4,\n",
    "    #     \"model_name\":llm_gpt4.model_name,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_GPT4_TURBO,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_GPT4_TURBO\n",
    "    # },\n",
    "    # \"llm_cohere_commandr\": {\n",
    "    #     \"model\": llm_cohere_commandr,\n",
    "    #     \"model_name\":llm_cohere_commandr.model,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_COMMANDR,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_COMMANDR\n",
    "    # },\n",
    "    # \"llm_cohere_commandr_plus\": {\n",
    "    #     \"model\": llm_cohere_commandr_plus,\n",
    "    #     \"model_name\":llm_cohere_commandr_plus.model,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_COMMANDR_PLUS,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_COMMANDR_PLUS\n",
    "    # }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking for the discussion is a NLE on it's own.\n",
    "Variations:\n",
    "- with the result only, and \n",
    "- with the result + discussion, \n",
    "- with the discussion + results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('system_prompt.txt', 'r') as file:\n",
    "    system_prompt = file.read()\n",
    "\n",
    "\n",
    "with open('user_prompt.txt', 'r') as file:\n",
    "    user_prompt = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI assistant acting as a healthcare professional tasked with analyzing complex clinical cases and selecting the most appropriate treatment option. You will be presented with a clinical case and a set of options. Your role is to:\n",
      "\n",
      "1. Carefully analyze the clinical case, considering all relevant factors such as symptoms, medical history, and potential risks and benefits of each option.\n",
      "2. Select the most appropriate option from those provided.\n",
      "3. Provide a concise explanation for your decision.\n",
      "\n",
      "Remember:\n",
      "- Only use the options provided (A, B, C, or D).\n",
      "- Base your decision solely on the information provided in the clinical case.\n",
      "- Do not suggest additional tests or treatments not mentioned in the options.\n",
      "- Your response should be in a specific format, starting with the chosen option letter, followed by a brief explanation.\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please analyze the following clinical case and select the most appropriate option:\n",
      "\n",
      "<clinical_case>\n",
      "{{CLINICAL_CASE}}\n",
      "</clinical_case>\n",
      "\n",
      "Consider these options:\n",
      "\n",
      "<options>\n",
      "{{OPTIONS}}\n",
      "</options>\n",
      "\n",
      "Provide your answer in this format:\n",
      "\n",
      "<answer>\n",
      "[A/B/C/D]\n",
      "I've chosen this option because... [Provide a three-sentence explanation]\n",
      "</answer>\n"
     ]
    }
   ],
   "source": [
    "print(user_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>case</th>\n",
       "      <th>opa</th>\n",
       "      <th>opb</th>\n",
       "      <th>opc</th>\n",
       "      <th>opd</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>answer_idx</th>\n",
       "      <th>answer</th>\n",
       "      <th>explanation</th>\n",
       "      <th>field</th>\n",
       "      <th>clinical_question</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://jamanetwork.com/journals/jamadermatolo...</td>\n",
       "      <td>A man in his 30s with AIDS presented with acut...</td>\n",
       "      <td>Herpes simplex virus</td>\n",
       "      <td>Histoplasmosis</td>\n",
       "      <td>Molluscum contagiosum</td>\n",
       "      <td>Mpox</td>\n",
       "      <td>D. Mpox</td>\n",
       "      <td>D</td>\n",
       "      <td>Mpox</td>\n",
       "      <td>The photographs demonstrate a Tzanck smear usi...</td>\n",
       "      <td>JAMA Dermatology Clinicopathological Challenge</td>\n",
       "      <td>What Is Your Diagnosis?</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://jamanetwork.com/journals/jama/fullarti...</td>\n",
       "      <td>An 80-year-old man with stage II bladder carci...</td>\n",
       "      <td>Perform a bone marrow biopsy</td>\n",
       "      <td>Prescribe all-trans retinoic acid</td>\n",
       "      <td>Repeat complete blood cell count with differen...</td>\n",
       "      <td>Start cytoreductive therapy with hydroxyurea</td>\n",
       "      <td>Granulocyte colony-stimulating factor (G-CSF)–...</td>\n",
       "      <td>C</td>\n",
       "      <td>Repeat complete blood cell count with differen...</td>\n",
       "      <td>The key to the correct diagnosis is recognizin...</td>\n",
       "      <td>JAMA Clinical Challenge</td>\n",
       "      <td>Three days after admission, his white blood ce...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://jamanetwork.com/journals/jamaneurology...</td>\n",
       "      <td>A 68-year-old man presented with progressive r...</td>\n",
       "      <td>Primary leptomeningeal lymphoma</td>\n",
       "      <td>Tolosa-Hunt syndrome</td>\n",
       "      <td>Perineural spread of cutaneous malignancy</td>\n",
       "      <td>Sphenoid wing meningioma</td>\n",
       "      <td>C. Perineural spread of cutaneous malignancy</td>\n",
       "      <td>C</td>\n",
       "      <td>Perineural spread of cutaneous malignancy</td>\n",
       "      <td>The MRI of the brain and orbits revealed asymm...</td>\n",
       "      <td>JAMA Neurology Clinical Challenge</td>\n",
       "      <td>What Is Your Diagnosis?</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://jamanetwork.com/journals/jamaoncology/...</td>\n",
       "      <td>A 31-year-old man presented with left cervical...</td>\n",
       "      <td>Kimura disease</td>\n",
       "      <td>Classic Hodgkin lymphoma</td>\n",
       "      <td>T-cell acute lymphoblastic lymphoma/leukemia</td>\n",
       "      <td>Myeloid/lymphoid neoplasms with eosinophilia a...</td>\n",
       "      <td>D. Myeloid/lymphoid neoplasms with eosinophili...</td>\n",
       "      <td>D</td>\n",
       "      <td>Myeloid/lymphoid neoplasms with eosinophilia a...</td>\n",
       "      <td>The differential diagnoses in young men with e...</td>\n",
       "      <td>JAMA Oncology Clinical Challenge</td>\n",
       "      <td>C, PET/CT 12 weeks after treatment initiation....</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://jamanetwork.com/journals/jamaotolaryng...</td>\n",
       "      <td>A 28-year-old woman presented with a 5-day his...</td>\n",
       "      <td>Lymphoma</td>\n",
       "      <td>Kikuchi-Fujimoto disease</td>\n",
       "      <td>Systemic lupus erythematosus</td>\n",
       "      <td>Rosai-Dorfman disease</td>\n",
       "      <td>B. Kikuchi-Fujimoto disease</td>\n",
       "      <td>B</td>\n",
       "      <td>Kikuchi-Fujimoto disease</td>\n",
       "      <td>Common diagnostic considerations of lymphadeno...</td>\n",
       "      <td>Clinical Challenge</td>\n",
       "      <td>What Is Your Diagnosis?</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://jamanetwork.com/journals/jamadermatolo...   \n",
       "1  https://jamanetwork.com/journals/jama/fullarti...   \n",
       "2  https://jamanetwork.com/journals/jamaneurology...   \n",
       "3  https://jamanetwork.com/journals/jamaoncology/...   \n",
       "4  https://jamanetwork.com/journals/jamaotolaryng...   \n",
       "\n",
       "                                                case  \\\n",
       "0  A man in his 30s with AIDS presented with acut...   \n",
       "1  An 80-year-old man with stage II bladder carci...   \n",
       "2  A 68-year-old man presented with progressive r...   \n",
       "3  A 31-year-old man presented with left cervical...   \n",
       "4  A 28-year-old woman presented with a 5-day his...   \n",
       "\n",
       "                               opa                                opb  \\\n",
       "0             Herpes simplex virus                     Histoplasmosis   \n",
       "1     Perform a bone marrow biopsy  Prescribe all-trans retinoic acid   \n",
       "2  Primary leptomeningeal lymphoma               Tolosa-Hunt syndrome   \n",
       "3                   Kimura disease           Classic Hodgkin lymphoma   \n",
       "4                         Lymphoma           Kikuchi-Fujimoto disease   \n",
       "\n",
       "                                                 opc  \\\n",
       "0                              Molluscum contagiosum   \n",
       "1  Repeat complete blood cell count with differen...   \n",
       "2          Perineural spread of cutaneous malignancy   \n",
       "3       T-cell acute lymphoblastic lymphoma/leukemia   \n",
       "4                       Systemic lupus erythematosus   \n",
       "\n",
       "                                                 opd  \\\n",
       "0                                               Mpox   \n",
       "1       Start cytoreductive therapy with hydroxyurea   \n",
       "2                           Sphenoid wing meningioma   \n",
       "3  Myeloid/lymphoid neoplasms with eosinophilia a...   \n",
       "4                              Rosai-Dorfman disease   \n",
       "\n",
       "                                           diagnosis answer_idx  \\\n",
       "0                                            D. Mpox          D   \n",
       "1  Granulocyte colony-stimulating factor (G-CSF)–...          C   \n",
       "2       C. Perineural spread of cutaneous malignancy          C   \n",
       "3  D. Myeloid/lymphoid neoplasms with eosinophili...          D   \n",
       "4                        B. Kikuchi-Fujimoto disease          B   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                               Mpox   \n",
       "1  Repeat complete blood cell count with differen...   \n",
       "2          Perineural spread of cutaneous malignancy   \n",
       "3  Myeloid/lymphoid neoplasms with eosinophilia a...   \n",
       "4                           Kikuchi-Fujimoto disease   \n",
       "\n",
       "                                         explanation  \\\n",
       "0  The photographs demonstrate a Tzanck smear usi...   \n",
       "1  The key to the correct diagnosis is recognizin...   \n",
       "2  The MRI of the brain and orbits revealed asymm...   \n",
       "3  The differential diagnoses in young men with e...   \n",
       "4  Common diagnostic considerations of lymphadeno...   \n",
       "\n",
       "                                             field  \\\n",
       "0  JAMA Dermatology Clinicopathological Challenge    \n",
       "1                         JAMA Clinical Challenge    \n",
       "2               JAMA Neurology Clinical Challenge    \n",
       "3                JAMA Oncology Clinical Challenge    \n",
       "4                              Clinical Challenge    \n",
       "\n",
       "                                   clinical_question  gender  \n",
       "0                            What Is Your Diagnosis?    male  \n",
       "1  Three days after admission, his white blood ce...    male  \n",
       "2                            What Is Your Diagnosis?    male  \n",
       "3  C, PET/CT 12 weeks after treatment initiation....    male  \n",
       "4                            What Is Your Diagnosis?  female  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/data/jama_pp.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'case', 'opa', 'opb', 'opc', 'opd', 'diagnosis', 'answer_idx',\n",
       "       'answer', 'explanation', 'field', 'clinical_question', 'gender'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A man in his 30s with AIDS presented with acute-onset painful scattered umbilicated papulopustules and ovoid ulcerated plaques with elevated, pink borders on the face, trunk, and extremities (Figure, A). The patient also had a new-onset cough but was afebrile and denied other systemic symptoms. Due to his significant immunocompromise, the clinical presentation was highly suspicious for infection. For rapid bedside differentiation of multiple infectious etiologies, a Tzanck smear was performed by scraping the base of an ulcerated lesion and inner aspect of a pseudopustule and scraping its base with a #15 blade. These contents were placed on a glass slide, fixed, and stained with Wright-Giemsa and subsequently Papanicolaou staining to further characterize the changes seen.A, Clinical image demonstrating papulopustules and ovoid ulcerated plaques with elevated, pink borders on the elbows. B, Tzanck smear using Wright-Giemsa staining of specimen demonstrating ballooning of keratinocytes and peripheralization of nuclear material (original magnification ×20).\n"
     ]
    }
   ],
   "source": [
    "print(df['case'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(llm, system_prompt,case,question):\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "      (\"system\", system_prompt),\n",
    "      (\"user\", user_prompt)\n",
    "  ])\n",
    "    chain = prompt | llm\n",
    "    # Run the LLM\n",
    "    start_time = time.time() #!TODO change if the prompt changes\n",
    "    response = chain.invoke({\"OPTIONS\": question, \"CLINICAL_CASE\": case})\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Time evaluation\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return response.content, time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_llms_and_df(llms, df):\n",
    "    # Create df_results as a copy of df\n",
    "    df_results = df.copy()\n",
    "\n",
    "    # Initialization\n",
    "    total_rows = len(df)\n",
    "    progress_interval = max(1, total_rows // 10)  # Calculate 10% of total rows\n",
    "\n",
    "    # LLM loop\n",
    "    for llm_name, llm_data in llms.items():\n",
    "        print(f\"\\nProcessing with LLM: {llm_name}\")  # Print current LLM being processed\n",
    "        \n",
    "        # Create new columns for this LLM's responses and times\n",
    "        df_results[f'{llm_name}_response'] = ''\n",
    "        df_results[f'{llm_name}_time'] = 0.0\n",
    "\n",
    "        # df loop\n",
    "        for idx_val, row_val in df_results.iterrows():\n",
    "            # Extracting the data\n",
    "            case = row_val['case']\n",
    "            question = f\"{row_val['clinical_question']}\\nA. {row_val['opa']}\\nB. {row_val['opb']}\\nC. {row_val['opc']}\\nD. {row_val['opd']}\\nE. {row_val['ope']}\"\n",
    "            print(question)\n",
    "\n",
    "            # Run the LLM\n",
    "            response, time_taken = run_llm(llm=llm_data[\"model\"], case=case, question=question)\n",
    "            \n",
    "            # Store results in df_results\n",
    "            df_results.at[idx_val, f'{llm_name}_response'] = response\n",
    "            df_results.at[idx_val, f'{llm_name}_time'] = time_taken\n",
    "\n",
    "            # OPTIONAL: Print results\n",
    "            print(response)\n",
    "            print(f\"Time taken: {time_taken} seconds\")\n",
    "\n",
    "            # Print progress every 10%\n",
    "            if (idx_val + 1) % progress_interval == 0:\n",
    "                progress_percentage = ((idx_val + 1) / total_rows) * 100\n",
    "                print(f\"Progress: {progress_percentage:.1f}% complete\")\n",
    "\n",
    "        print(f\"Finished processing with LLM: {llm_name}\")  # Print when finished with current LLM\n",
    "\n",
    "    print(\"\\nAll LLMs processed. Returning results.\")\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(df, llms, file_name, experiment_name, experiment_description):\n",
    "    \"\"\"\n",
    "    Run an experiment with the given dataframe and LLMs, saving results to a new folder.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input dataframe to process\n",
    "    llms (dict): Dictionary of LLMs to use in the experiment\n",
    "    file_name (str): Name to use for output files\n",
    "    experiment_name (str): Name of the experiment\n",
    "    experiment_description (str): Description of the experiment\n",
    "\n",
    "    Returns:\n",
    "    str: Path to the experiment folder\n",
    "    \"\"\"\n",
    "    # Step 1: Create a folder \n",
    "    folder_path = os.path.join(os.getcwd(), file_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Step 2: Save the original df in the folder as `input_data`\n",
    "    input_data_path = os.path.join(folder_path, 'input_data.csv')\n",
    "    df.to_csv(input_data_path, index=False)\n",
    "\n",
    "    # Step 3: Run process_llms_and_df and save the outputs in the folder\n",
    "    results = process_llms_and_df(llms, df)\n",
    "    output_path = os.path.join(folder_path, f\"{file_name}_output.csv\")\n",
    "    results.to_csv(output_path, index=False)\n",
    "\n",
    "    # Step 4: Save a .txt file of the EXPERIMENT_DESCRIPTION\n",
    "    experiment_description_path = os.path.join(folder_path, 'experiment_description.txt')\n",
    "    with open(experiment_description_path, 'w') as f:\n",
    "        f.write(experiment_description)\n",
    "\n",
    "    print(f\"Experiment '{experiment_name}' saved successfully.\")\n",
    "    \n",
    "    return folder_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "male      5\n",
      "female    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Number of examples to try\n",
    "NUMBER_EXAMPLES_TRY = 10\n",
    "\n",
    "# Perform stratified sampling\n",
    "df_gender, _ = train_test_split(df, test_size=len(df) - NUMBER_EXAMPLES_TRY, stratify=df['gender'], random_state=1)\n",
    "\n",
    "# Check the value counts in the sampled data\n",
    "print(df_gender['gender'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the spaCy English model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def augment_and_combine_data(df_gender):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Define the replacements dictionary\n",
    "    replacements = {\n",
    "        'man': ['woman', 'patient'],\n",
    "        'boy': ['girl', 'child'],\n",
    "        'male': ['female', 'patient'],\n",
    "        'he': ['she', 'they'],\n",
    "        'his': ['her', 'their'],\n",
    "        'him': ['her', 'them'],\n",
    "        'himself': ['herself', 'themselves'],\n",
    "        'woman': ['man', 'patient'],\n",
    "        'girl': ['boy', 'child'],\n",
    "        'female': ['male', 'patient'],\n",
    "        'she': ['he', 'they'],\n",
    "        'her': ['his', 'their'],\n",
    "        'hers': ['his', 'theirs'],\n",
    "        'herself': ['himself', 'themselves']\n",
    "    }\n",
    "\n",
    "    def replace_gender(text, version):\n",
    "        doc = nlp(text)\n",
    "        modified_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() in replacements:\n",
    "                if version == 'augmented_binary':\n",
    "                    modified_tokens.append(replacements[token.text.lower()][0])\n",
    "                elif version == 'augmented_nonbinary':\n",
    "                    modified_tokens.append(replacements[token.text.lower()][1])\n",
    "                else:\n",
    "                    modified_tokens.append(token.text)\n",
    "            else:\n",
    "                modified_tokens.append(token.text)\n",
    "        return ' '.join(modified_tokens)\n",
    "\n",
    "    # Enumerate cases\n",
    "    df_gender['case_id'] = df_gender.index + 1\n",
    "    df_gender['version'] = 'original'\n",
    "\n",
    "    # Apply the function to create new vignettes\n",
    "    df_augmented = df_gender.copy()\n",
    "    df_augmented['case'] = df_augmented['case'].apply(lambda x: replace_gender(x, 'augmented_binary'))\n",
    "    df_augmented['version'] = 'augmented_binary'\n",
    "    df_augmented['gender'] = df_augmented['gender'].map({'male': 'female', 'female': 'male', 'non-binary': 'non-binary'})\n",
    "\n",
    "    df_gender_neutral = df_gender.copy()\n",
    "    df_gender_neutral['case'] = df_gender_neutral['case'].apply(lambda x: replace_gender(x, 'augmented_nonbinary'))\n",
    "    df_gender_neutral['version'] = 'augmented_nonbinary'\n",
    "    df_gender_neutral['gender'] = 'non-binary'\n",
    "\n",
    "    # Concatenate the original DataFrame with the augmented and gender-neutral DataFrames\n",
    "    df_combined = pd.concat([df_gender, df_augmented, df_gender_neutral], ignore_index=True)\n",
    "\n",
    "    return df_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "male          10\n",
      "female        10\n",
      "non-binary    10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_test = augment_and_combine_data(df_gender)\n",
    "print(df_test['gender'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['link', 'case', 'opa', 'opb', 'opc', 'opd', 'diagnosis', 'answer_idx',\n",
       "       'answer', 'explanation', 'field', 'clinical_question', 'gender',\n",
       "       'case_id', 'version'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# # Assuming df_gender is your DataFrame\n",
    "# modified_questions = df_test[df_test['version'] == 'original']\n",
    "\n",
    "# for i in range(len(modified_questions)):\n",
    "#     print(f\"Case {i+1} of {len(modified_questions)}\")\n",
    "#     print(f\"Version: {modified_questions['version'].iloc[i]}\")\n",
    "#     print(f\"Gender: {modified_questions['gender'].iloc[i]}\")\n",
    "#     print(f\"The question is: {modified_questions['question'].iloc[i]}\")\n",
    "#     print(\"----\")\n",
    "#     time.sleep(1)  # Pauses for 1 second between prints. Adjust the sleep time as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of these experiments is to assess the difference of performance and explanation for different social groups: Gender and Ethnic Group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gender differentiation, cases were gender is not relevant were selected, meaning no gynecology, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== EXPERIMENT SETTING\n",
    "df = df_test  # Assuming df_test is your input DataFrame\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "experiment_name = \"Testingwith10\"\n",
    "file_name = f\"{today}_{experiment_name}\"\n",
    "experiment_description = \"Description of the experiment\"\n",
    "\n",
    "\n",
    "# =========================== RUNNING THE EXPERIMENT\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "folder_path = run_experiment(df=df, llms=llms, file_name=file_name, experiment_name=experiment_name, experiment_description=experiment_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "- make sure it's all in caps when needed and the replacement is efficient\n",
    "- correct the punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnic experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ethnic evaluation, cases were ethnicity is non-relevant were selected, so no gynecology, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
