{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM API connection\n",
    "Describe the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: AzureOpenAI in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (0.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from AzureOpenAI) (3.9.5)\n",
      "Requirement already satisfied: jsonschema in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from AzureOpenAI) (4.22.0)\n",
      "Requirement already satisfied: requests in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from AzureOpenAI) (2.32.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from aiohttp->AzureOpenAI) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from aiohttp->AzureOpenAI) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from aiohttp->AzureOpenAI) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from aiohttp->AzureOpenAI) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from aiohttp->AzureOpenAI) (1.9.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from jsonschema->AzureOpenAI) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from jsonschema->AzureOpenAI) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from jsonschema->AzureOpenAI) (0.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from requests->AzureOpenAI) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from requests->AzureOpenAI) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from requests->AzureOpenAI) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenzabenkirane/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages (from requests->AzureOpenAI) (2024.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ========== Installs\n",
    "%pip install python-dotenv\n",
    "%pip install AzureOpenAI\n",
    "%pip install --upgrade --quiet  langchain-openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Imports and general settings\n",
    "\n",
    "# --- General settings\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Python basics\n",
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import random\n",
    "\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import typing as tp\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import json\n",
    "import time\n",
    "\n",
    "# --- ML basics\n",
    "\n",
    "\n",
    "# --- LLMs basics\n",
    "# Import Azure OpenAI\n",
    "from langchain_openai import AzureOpenAI\n",
    "# from openai import AzureOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs costs\n",
    "\n",
    "# == GPTs\n",
    "# --- GPT 3.5\n",
    "PRICE_PER_INPUT_TOKEN_GPT3=0.5/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT3=1.5/(1e6)\n",
    "# --- GPT 4o\n",
    "PRICE_PER_INPUT_TOKEN_GPT4O=5/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4O=15/(1e6)\n",
    "# --- GPT 4 Turbo\n",
    "PRICE_PER_INPUT_TOKEN_GPT4_TURBO=10/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4_TURBO=30/(1e6)\n",
    "# ---GPT 4\n",
    "PRICE_PER_INPUT_TOKEN_GPT4=30/(1e6) #Cheapest version\n",
    "PRICE_PER_OUTPUT_TOKEN_GPT4=60/(1e6)\n",
    "\n",
    "# == Cohere\n",
    "# --- Command-R\n",
    "PRICE_PER_INPUT_TOKEN_COMMANDR=0.5/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_COMMANDR=1.5/(1e6)\n",
    "# --- Command-R-Plus\n",
    "PRICE_PER_INPUT_TOKEN_COMMANDR_PLUS=3/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_COMMANDR_PLUS=15/(1e6)\n",
    "\n",
    "# == Claude\n",
    "# --- Haiku\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_HAIKU=0.25/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_HAIKU=1.25/(1e6)\n",
    "# --- Sonnet\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_SONNET=3/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_SONNET=15/(1e6)\n",
    "\n",
    "# --- Opus\n",
    "PRICE_PER_INPUT_TOKEN_CLAUDE_OPUS=15/(1e6)\n",
    "PRICE_PER_OUTPUT_TOKEN_CLAUDE_OPUS=75/(1e6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Get the API keys #! TODO: FIX THIS!!!!\n",
    "\n",
    "# # --- Load the environment variables\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv() \n",
    "# BIAS1_KEY1 = os.getenv('BIAS1_KEY1')\n",
    "\n",
    "# --- Set the environment variables\n",
    "# OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-12-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://bias1.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"2da35c17af2945579bae74d83d631b2f\" #bias1\n",
    "\n",
    "# Other LLMs\n",
    "os.environ[\"MISTRAL_API_KEY\"]=\"Q1LAALAKpliBWH0Wch3scX6gH6DzKzB1\" #bias1\n",
    "os.environ[\"ANTHROPIC_API_KEY\"]=\"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\" #bias1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs hyperparameters\n",
    "TEMPERATURE=0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLMs loading\n",
    "llm_gpt3 = AzureOpenAI(\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    openai_api_key=AZURE_OPENAI_KEY,\n",
    "    temperature=TEMPERATURE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== LLM dictionnary definition\n",
    "\n",
    "llms = {\n",
    "    \"llm_gpt3\": {\n",
    "        \"model\": llm_gpt3,\n",
    "        \"model_name\":llm_gpt3.model_name,\n",
    "        \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_GPT3,\n",
    "        \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_GPT3\n",
    "    }\n",
    "    # \"llm_gpt4\": {\n",
    "    #     \"model\": llm_gpt4,\n",
    "    #     \"model_name\":llm_gpt4.model_name,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_GPT4_TURBO,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_GPT4_TURBO\n",
    "    # },\n",
    "    # \"llm_cohere_commandr\": {\n",
    "    #     \"model\": llm_cohere_commandr,\n",
    "    #     \"model_name\":llm_cohere_commandr.model,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_COMMANDR,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_COMMANDR\n",
    "    # },\n",
    "    # \"llm_cohere_commandr_plus\": {\n",
    "    #     \"model\": llm_cohere_commandr_plus,\n",
    "    #     \"model_name\":llm_cohere_commandr_plus.model,\n",
    "    #     \"price_per_input_token\": PRICE_PER_INPUT_TOKEN_COMMANDR_PLUS,\n",
    "    #     \"price_per_output_token\": PRICE_PER_OUTPUT_TOKEN_COMMANDR_PLUS\n",
    "    # }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are a healthcare professional and you'll be presented with a complex clinical case. Your role is to select one of the possible options for that patient.\"\n",
    "user_prompt=\"A 54-year-old woman was evaluated for a rash on her face, trunk, arms, and proximal thighs that developed 10 months prior to presentation. She had arthralgias and fatigue and reported transient oral ulcers on her buccal mucosa. The patient had false-positive syphilis test results during 2 pregnancies, 25 and 22 years prior, based on a positive blood rapid plasma reagin (RPR) test result (titer, 1:2 [reference, nonreactive]) and negative fluorescent treponemal antibody (FTA-ABS) test result. On current presentation, she had erythematous annular and indurated plaques on her face, trunk, and extremities (Supplement). Laboratory testing revealed positive cardiolipin IgM (150 U/mL [reference, <13 U/mL]) and cardiolipin IgG (40 U/mL [reference, <15 U/mL]) antibodies. RPR titer was 1:1024. Skin biopsy revealed a dense perivascular and periadnexal infiltrate of lymphocytes and plasma cells. The patient was not taking any new prescriptions or over-the-counter medications and did not use herbal supplements. She had no history of sexually transmitted infections and was sexually active with 1 male partner for the past 12 months \\n *How Do You Interpret These Test Results?* \\n A. Primary syphilis is likely. \\n B. Secondary syphilis is likely. \\n C. The rapid plasma reagin is a false-positive result due to cardiolipin antibodies. \\n D. The rapid plasma reagin is a false-positive result from prior pregnancies.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking for the discussion is a NLE on it's own.\n",
    "Variations:\n",
    "- with the result only, and \n",
    "- with the result + discussion, \n",
    "- with the discussion + results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_llm(llm,system_prompt,user_prompt):\n",
    "  prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", user_prompt)\n",
    "    ])\n",
    "  chain = prompt | llm\n",
    "  start_time = time.time()\n",
    "  response=chain.invoke(prompt)\n",
    "  end_time = time.time()\n",
    "  time_taken = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "  return response, time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected mapping type as input to ChatPromptTemplate. Received <class 'langchain_core.prompts.chat.ChatPromptTemplate'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response, time_taken\u001b[38;5;241m=\u001b[39m\u001b[43mrun_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime_taken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m, in \u001b[0;36mrun_llm\u001b[0;34m(llm, system_prompt, user_prompt)\u001b[0m\n\u001b[1;32m      6\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m llm\n\u001b[1;32m      7\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m response\u001b[38;5;241m=\u001b[39m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     10\u001b[0m time_taken \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:2502\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2498\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m   2499\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2500\u001b[0m )\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2502\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/prompts/base.py:151\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[1;32m    150\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/runnables/base.py:1598\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1594\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1595\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1596\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1597\u001b[0m         Output,\n\u001b[0;32m-> 1598\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1600\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1601\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1606\u001b[0m     )\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1608\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/runnables/config.py:380\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    379\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/prompts/base.py:134\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[0;32m--> 134\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[0;32m~/Desktop/GitHub/24ucl_thesis/thesis_clinical_llm_bias/.conda/lib/python3.11/site-packages/langchain_core/prompts/base.py:120\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    117\u001b[0m         inner_input \u001b[38;5;241m=\u001b[39m {var_name: inner_input}\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected mapping type as input to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(inner_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected mapping type as input to ChatPromptTemplate. Received <class 'langchain_core.prompts.chat.ChatPromptTemplate'>."
     ]
    }
   ],
   "source": [
    "response, time_taken=run_llm(llm,system_prompt,user_prompt)\n",
    "print(response)\n",
    "print(f\"Time taken: {time_taken} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = AzureOpenAI(\n",
    "#   azure_endpoint = AZURE_OPENAI_ENDPOINT,\n",
    "#   api_key=AZURE_OPENAI_API_KEY,\n",
    "#   api_version=\"2024-02-15-preview\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureOpenAI(deployment_name=AZURE_OPENAI_CHATGPT_DEPLOYMENT, temperature=0.3, openai_api_key=AZURE_OPENAI_KEY)\n",
    "\n",
    "llm_prompt = PromptTemplate(\n",
    "    input_variables=[\"human_prompt\"],\n",
    "    template=\"The following is a conversation with an AI assistant. The assistant is helpful.\\n\\nAI: I am an AI created by OpenAI. How can I help you today?\\nHuman: {human_prompt}?\",\n",
    ")\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=llm_prompt)\n",
    "\n",
    "return chain.run(prompt) # prompt is human input from request body\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALLS\n",
    "%pip install -U langchain\n",
    "%pip install -U langsmith\n",
    "%pip install -U langchain-core\n",
    "\n",
    "# ---- OpenAI\n",
    "%pip install langchain-openai\n",
    "\n",
    "# ---- Cohere\n",
    "%pip install -U langchain-cohere\n",
    "\n",
    "# # ---- Mistral\n",
    "%pip install -U langchain-mistralai\n",
    "\n",
    "# # ---- Anthropic\n",
    "%pip install langchain-anthropic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM models import and API\n",
    "\n",
    "\n",
    "## GPT -------- AZURE\n",
    "\n",
    "\n",
    "# AZURE_OPENAI_API_KEY_US = \"\"\n",
    "# AZURE_OPENAI_ENDPOINT_US = \"\"\n",
    "\n",
    "# client_us = AzureOpenAI(\n",
    "#   azure_endpoint = AZURE_OPENAI_ENDPOINT_US,\n",
    "#   api_key=AZURE_OPENAI_API_KEY_US,\n",
    "#   api_version=\"2024-02-15-preview\"\n",
    "# )\n",
    "\n",
    "## GPT -------- OPENAI\n",
    "# from openai import OpenAI\n",
    "# from google.colab import userdata\n",
    "# OPENAI_API_KEY = userdata.get('gmail_l_halomi2')\n",
    "# client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Definte the prompting experiment for each line\n",
    "\n",
    "\n",
    "def prompting_experiments_gpt(prompt,label_type,num_ex,order_ex,examples,mt_text, src_text):\n",
    "\n",
    "  # ------------------------ CHAT HISTORY\n",
    "  # ------- Initial prompt\n",
    "  # Initialise chat history with original prompt -> in preamble\n",
    "  chat_history=[]\n",
    "  # ------- Optional: Adding examples when more than zero-shot\n",
    "  if num_ex > 0:\n",
    "    for example in examples:\n",
    "        src_text_ex, mt_text_ex, label_ex = example\n",
    "        chat_history.append({\"role\": \"user\", \"content\": f\"Source Text:\\n{{{{{src_text_ex}}}}}\\nTranslated Text:\\n{{{{{mt_text_ex}}}}}\\nAnswer:\"})\n",
    "        chat_history.append({\"role\": \"user\", \"content\": label_ex})\n",
    "\n",
    "\n",
    "  # ------------------------ PROMPTING MESSAGE\n",
    "  # ------- Message\n",
    "  message = f\"Source Text:\\n'{mt_text}'\\nTranslated Text: '{src_text}'\\nAnswer:\"\n",
    "\n",
    "  if len(chat_history)>0:\n",
    "    messages = [{\"role\":\"user\",\"content\":prompt+'\\n'+chat_history[0][\"content\"]}]\n",
    "    messages.extend(chat_history[1:])\n",
    "    messages.append({\"role\":\"user\",\"content\":message})\n",
    "  else:\n",
    "    messages = [{\"role\":\"user\",\"content\":prompt}]\n",
    "    messages.append({\"role\":\"user\",\"content\":message})\n",
    "\n",
    "  return messages\n",
    "  # -------  Run the API\n",
    "  # try:\n",
    "  #   start_time=time.time()\n",
    "  #   response = client.chat.completions.create(\n",
    "  #       model=\"gpt35-turbo\",\n",
    "  #       messages = messages,\n",
    "  #       temperature=0,\n",
    "  #       logprobs=True,\n",
    "  #       top_logprobs=2)\n",
    "  #   end_time=time.time()\n",
    "  #   elapsed_time=end_time-start_time\n",
    "\n",
    "  #   classification = response.choices[0].message.content\n",
    "  # except Exception as e:\n",
    "  #   error_message = str(e)\n",
    "  #   classification = 'Error'\n",
    "  #   elapsed_time = None\n",
    "  #   response = error_message\n",
    "\n",
    "  # return classification, elapsed_time, response\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
